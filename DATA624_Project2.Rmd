---
title: "DATA624 - Project 2"
author: "Glen Dale Davis"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages:

```{r packages, warning = FALSE, message = FALSE}
library(tidyverse)
library(httr)
library(readxl)
library(DataExplorer)
library(psych)
library(knitr)
library(snakecase)
library(RColorBrewer)
library(VIM)
library(ggcorrplot)

```

```{r }
cur_theme <- theme_set(theme_classic())
palette <- brewer.pal(n = 12, name = "Paired")

```

## Overview:

New regulations require ABC Beverage to understand our manufacturing process and the predictive factors. We need to be able to report to leadership our predictive model of `PH`.

We load the historical dataset provided.

```{r historical_data}
my_url1 <- "https://github.com/geedoubledee/data624_project2/raw/main/StudentData.xlsx"
temp <- tempfile(fileext = ".xlsx")
req <- GET(my_url1, authenticate(Sys.getenv("GITHUB_PAT"), ""),
           write_disk(path = temp))
main_df <- readxl::read_excel(temp)
colnames(main_df) <- to_screaming_snake_case(colnames(main_df))

```

We take a look at the distribution for the response variable and a summary of it.

```{r response_distribution, warning = FALSE, message = FALSE}
annotations <- data.frame(x = c(min(main_df$PH, na.rm = TRUE),
                                round(median(main_df$PH, na.rm = TRUE), 2),
                                max(main_df$PH, na.rm = TRUE)),
                          y = c(20, 340, 20),
                          label = c("Min:", "Median:", "Max:"))
p0 <- main_df |>
    ggplot(aes(x = PH)) +
    geom_histogram(binwidth = 0.05, color = "#1F78B4", fill = "#A6CEE3") +
    geom_text(data = annotations,
              aes(x = x, y = y, label = paste(label, x)),
              size = 4, fontface = "bold") +
    scale_x_continuous(limits = c(7.5, 9.5), breaks = seq(7.5, 9.5, 0.5)) +
    scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 25))
p0

```

```{r response_summary}
summary(main_df$PH)

```

There are 4 observations with missing PH values. This is a small enough percentage of our total observations to justify simple list-wise deletion. We lose little by removing these observations, and we would gain little by imputing them.

```{r }
main_df <- main_df |>
    filter(!is.na(PH))

```

We take a look at histograms for the numeric predictor variables in batches since there are so many of them.

```{r predictors_distributions1, warning = FALSE, message = FALSE}
non_numeric <- c("BRAND_CODE")
all_numeric <- colnames(main_df |> select(-all_of(c("PH", non_numeric))))
n = 8
all_numeric_chunks <- split(all_numeric, ceiling(seq_along(all_numeric)/n))
pivot_df <- main_df |>
    select(-PH) |>
    pivot_longer(cols = all_of(all_numeric), names_to = "PREDICTOR",
                 values_to = "VALUE")
p1 <- pivot_df |>
    filter(PREDICTOR %in% all_numeric_chunks[[1]]) |>
    ggplot(aes(x = VALUE)) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "CARB_PRESSURE"),
                   binwidth = 2) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "CARB_TEMP"),
                   binwidth = 2) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "CARB_VOLUME"),
                   binwidth = 0.04) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "FILL_OUNCES"),
                   binwidth = 0.04) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "PC_VOLUME"),
                   binwidth = 0.02) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "PSC"),
                   binwidth = 0.02) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "PSC_CO_2"),
                   binwidth = 0.02) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "PSC_FILL"),
                   binwidth = 0.04) +
    facet_wrap(vars(PREDICTOR), ncol = 4, scales = "free_x") +
    labs(y = "COUNT",
         title = "Batch 1 Predictor Distributions") +
    theme(panel.spacing.x = unit(4, "mm"),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
          plot.title.position = "plot")
p1

```

In the first batch of numeric predictors, we see that `PSC`, `PSC_CO_2`, and `PSC_FILL` are all right-skewed, and the distribution for `CARB_VOLUME` is multimodal. The distributions for the rest of the variables are nearly normal.

```{r predictors_distributions2, warning = FALSE, message = FALSE}
p2 <- pivot_df |>
    filter(PREDICTOR %in% all_numeric_chunks[[2]]) |>
    ggplot(aes(x = VALUE)) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "CARB_PRESSURE_1"),
                   binwidth = 2) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "FILL_PRESSURE"),
                   binwidth = 2) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "FILLER_LEVEL"),
                   binwidth = 6) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "HYD_PRESSURE_1"),
                   binwidth = 4) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "HYD_PRESSURE_2"),
                   binwidth = 4) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "HYD_PRESSURE_3"),
                   binwidth = 4) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "HYD_PRESSURE_4"),
                   binwidth = 6) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "MNF_FLOW"),
                   binwidth = 20) +
    facet_wrap(vars(PREDICTOR), ncol = 4, scales = "free_x") +
    labs(y = "COUNT",
         title = "Batch 2 Predictor Distributions") +
    theme(panel.spacing.x = unit(4, "mm"),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
          plot.title.position = "plot")
p2

```

In the second batch of numeric predictors, we see that `HYD_PRESSURE_1`, `HYD_PRESSURE_2`, and `HYD_PRESSURE_3` are heavy with zero value observations, skewing their distributions. Most observations for `MNF_FLOW` are around -100, and its distribution might be degenerate. We'll check for degeneracy for this variable and any others shortly. `FILL_PRESSURE` and `FILLER_LEVEL` are multimodal. `HYD_PRESSURE_4` is right-skewed. `CARB_PRESSURE_1` has the only nearly normal distribution here.

```{r predictors_distributions3, warning = FALSE, message = FALSE}
p3 <- pivot_df |>
    filter(PREDICTOR %in% all_numeric_chunks[[3]]) |>
    ggplot(aes(x = VALUE)) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "BALLING"),
                   binwidth = 0.25) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "CARB_FLOW"),
                   binwidth = 400) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "DENSITY"),
                   binwidth = 0.1) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "FILLER_SPEED"),
                   binwidth = 200) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "MFR"),
                   binwidth = 50) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "PRESSURE_VACUUM"),
                   binwidth = 0.25) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "TEMPERATURE"),
                   binwidth = 1) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "USAGE_CONT"),
                   binwidth = 1) +
    facet_wrap(vars(PREDICTOR), ncol = 4, scales = "free_x") +
    labs(y = "COUNT",
         title = "Batch 3 Predictor Distributions") +
    theme(panel.spacing.x = unit(4, "mm"),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
          plot.title.position = "plot")
p3

```

In the third batch of numeric predictors, we see multimodal distributions for `BALLING`, `CARB_FLOW`, and `DENSITY`. `FILLER_SPEED`, `MFR`, and `USAGE_CONT` are left-skewed, and `TEMPERATURE` and `PRESSURE_VACUUM` are right-skewed.

```{r predictors_distributions4, warning = FALSE, message = FALSE}
p4 <- pivot_df |>
    filter(PREDICTOR %in% all_numeric_chunks[[4]]) |>
    ggplot(aes(x = VALUE)) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "AIR_PRESSURER"),
                   binwidth = 0.5) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "ALCH_REL"),
                   binwidth = 0.25) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "BALLING_LVL"),
                   binwidth = 0.25) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "BOWL_SETPOINT"),
                   binwidth = 10) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "CARB_REL"),
                   binwidth = 0.1) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "OXYGEN_FILLER"),
                   binwidth = 0.02) +
    geom_histogram(color = "#1F78B4", fill = "#A6CEE3",
                   data = subset(pivot_df, PREDICTOR == "PRESSURE_SETPOINT"),
                   bins = 8) +
    facet_wrap(vars(PREDICTOR), ncol = 4, scales = "free_x") +
    labs(y = "COUNT",
         title = "Batch 4 Predictor Distributions") +
    theme(panel.spacing.x = unit(4, "mm"),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
          plot.title.position = "plot")
p4

```

In the last batch of numeric predictors, we see that `AIR_PRESSURER` and `OXYGEN_FILLER` are right-skewed. The distributions for `ALCH_REL`, `BALLING_LVL`, and `PRESSURE_SETPOINT` are multimodal. `BOWL_SETPOINT` is left-skewed. `CARB_REL` is the only variable for which the distribution is nearly normal.

Summary statistics for all numeric predictors are below.

```{r predictors_summary}
remove <- c("n", "vars", "trimmed", "mad", "range", "se")
describe <- main_df |>
    select(all_of(all_numeric)) |>
    describe() |>
    select(-all_of(remove))
knitr::kable(describe, format = "simple")

```

Next we examine the dataset's completeness. 

```{r introduce_data}
remove <- c("discrete_columns", "continuous_columns", "total_observations",
            "memory_usage")
introduce <- main_df |>
    introduce() |>
    select(-all_of(remove))
knitr::kable(t(introduce), format = "simple")

```

Only 2,038 out of 2,571 rows are complete, which is about 79 percent of observations. There are 844 missing values. None of our variables are completely `NA`.

We take a closer look at where the missing values are.

```{r missing_values_plot1, include = FALSE}
p5 <- plot_missing(main_df, missing_only = TRUE,
                   ggtheme = theme_classic(), title = "Missing Values",
                   geom_label_args = list("size" = 2,
                                          "label.padding" = unit(0.1, "lines")))

```

```{r missing_values_plot2, warning = FALSE, message = FALSE}
p5 <- p5 + 
    scale_fill_brewer(palette = "Paired") +
    theme(plot.title.position = "plot")
p5

```

`MFR`, `BRAND_CODE`, and `FILLER_SPEED` are the predictors with the most missing values, but many other predictors are missing values as well. We coerce `BRAND_CODE` to a factor and add a level for `NA` values to handle missingness for this categorical predictor.

```{r brand_code}
main_df <- main_df |>
    mutate(BRAND_CODE = factor(BRAND_CODE, exclude = NULL))

```

We will choose either multiple imputation or knn imputation for the numeric predictors with missing data depending on which method performs better on our data. We will also create a secondary version of the data where we perform list-wise deletion instead.

Before we handle the remaining missing data, we first look at correlations between our predictors and the response variable. Because we have so many variables, it would be difficult to visualize all correlations at the same time without binning them. So we will bin absolute value correlations into four groups: 1) 0.00 to 0.25, 2) 0.26 to 0.50, 3) 0.51 to 0.75, and 4) 0.76 to 1.00. We won't visualize any correlations less than 0.26, but note that this doesn't imply those correlations are insignificant. Limiting what we examine most closely will simply help us hone in on a) the predictor variables we should expect any good model we develop to include and b) the predictor variables that are so highly correlated with one another that they could inhibit certain models' performance. 

```{r correlation_plot1}
incl <- c("PH", sort(colnames(main_df |> select(-PH))))
palette <- brewer.pal(n = 7, name = "RdBu")[c(1, 4, 7)]
r <- model.matrix(~0+., data = main_df |> select(all_of(incl))) |>
    cor(use = "pairwise.complete.obs")
is.na(r) <- abs(r) > 0.5
is.na(r) <- abs(r) < 0.26
p7 <- r |>
    ggcorrplot(show.diag = FALSE, type = "lower", lab = TRUE, lab_size = 2,
               tl.cex = 8, tl.srt = 90,
               colors = palette, outline.color = "white") +
    labs(title = "Correlations Between 0.26 and 0.50 (Absolute Value)") +
    theme(plot.title.position = "plot")
p7

```

Here, we see the predictors that are most positively correlated with `PH` are `BOWL_SETPOINT` and `FILLER_LEVEL`, and the predictors that are most negatively correlated with `PH` are `BRAND_CODE` level "C", `FILL_PRESSURE`, `HYD_PRESSURE_3`, `MNF_FLOW`, `PRESSURE_SETPOINT`, and `USAGE_CONT`. While some of the predictors in this plot are moderately correlated with each other, we will focus on higher/more worrisome predictor-predictor correlation levels in the following two plots.

```{r correlation_plot2}
r <- model.matrix(~0+., data = main_df |> select(all_of(incl))) |>
    cor(use = "pairwise.complete.obs")
is.na(r) <- abs(r) > 0.75
is.na(r) <- abs(r) < 0.51
p8 <- r |>
    ggcorrplot(show.diag = FALSE, type = "lower", lab = TRUE, lab_size = 2,
               tl.cex = 8, tl.srt = 90,
               colors = palette, outline.color = "white") +
    labs(title = "Correlations Between 0.51 and 0.75 (Absolute Value)") +
    theme(plot.title.position = "plot")
p8

```

Here, we notice immediately that `PH` is missing from the plot and is therefore not correlated with any predictors at a level between 0.51 and 0.75 in absolute value. Although we could comment on all these correlation levels, we see high (> 0.6) positive predictor-predictor correlations between:

* `MNF_FLOW` and `HYD_PRESSURE_2`

* `HYD_PRESSURE_3` and `HYD_PRESSURE_2`

* `HYD_PRESSURE_2` and `HYD_PRESSURE_1`

We also see high (< -0.6) negative predictor-predictor correlations between: 

* `PRESSURE_VACUUM` and `HYD_PRESSURE_3`/`HYD_PRESSURE_2`

* `HYD_PRESSURE_4` and `BRAND_CODE` level "D"/`ALCH_REL`

* `BRAND_CODE` level "B" and `DENSITY`/`BALLING_LVL`/`BALLING`/`ALCH_REL`

```{r correlation_plot3}
r <- model.matrix(~0+., data = main_df |> select(all_of(incl))) |>
    cor(use = "pairwise.complete.obs")
is.na(r) <- abs(r) < 0.76
p9 <- r |>
    ggcorrplot(show.diag = FALSE, type = "lower", lab = TRUE, lab_size = 2,
               tl.cex = 8, tl.srt = 90,
               colors = palette, outline.color = "white") +
    labs(title = "Correlations Between 0.76 and 1.00 (Absolute Value)") +
    theme(plot.title.position = "plot")
p9

```

`PH` is again missing, so it is therefore not correlated with any predictors at a level between 0.76 and 1.00 in absolute value. Although we could again comment on all these correlation levels, we see extremely high (> 0.9) positive predictor-predictor correlations between

* `MFR` and `FILLER_SPEED`

* `HYD_PRESSURE_3` and `HYD_PRESSURE_2`

* `FILLER_LEVEL` and `BOWL_SETPOINT`

* `DENSITY` and `BALLING_LVL`/`BALLING`/`ALCH_REL`

* `BRAND_CODE` level "D" and `ALCH_REL`

* `BALLING_LVL` and `BALLING`/`ALCH_REL`

* `BALLING` and `ALCH_REL`

There are no extremely high (< -0.9) negative predictor-predictor correlations.

When creating models that are not robust to collinearity later, we will definitely need to exclude one or more variables in each extremely correlated group, and we may have to do the same for less (but still highly) correlated groups as well.

Before we move on to imputing any missing predictor values or performing list-wise deletion, we set a seed and split the dataset into train and test sets. 

```{r train_test_split}
set.seed(417)
rows <- sample(nrow(main_df))
main_df <- main_df[rows, ]
sample <- sample(c(TRUE, FALSE), nrow(main_df), replace=TRUE,
                 prob=c(0.7,0.3))
train_df <- main_df[sample, ]
train_x <- train_df |>
    select(-PH)
train_y <- train_df$PH
test_df <- main_df[!sample, ]
test_x <- test_df |>
    select(-PH)
test_y <- test_df$PH

```

**TK: We impute the missing values in the numeric predictors for the train and test sets separately. First, we check whether multiple imputation or knn imputation performs better on our data using a subset of the data where we've introduced random missingness. By doing this, we can compare imputed values to actual values for both methods and measure their accuracy.**

```{r impute_missing_values}
find_cols_na <- function(df){
    col_sums_na <- colSums(is.na(df))
    cols <- names(col_sums_na[col_sums_na > 0])
    print(paste("Columns with Missing Values:", length(cols)))
    cols #returns column names vector
}


```

We create secondary train and test sets where all observations with any missing numeric predictor values have been removed. 

```{r list-wise_deletion}
remove_rows_na <- function(df){
    na_row_sums <- rowSums(is.na(df))
    row_has_na <- ifelse(na_row_sums > 0, TRUE, FALSE)
    copy <- df[!row_has_na, ]
    print(paste("Rows with Missing Values Removed:",
                sum(na_row_sums > 0)))
    copy
}
secondary_train_df <- remove_rows_na(train_df)
secondary_train_x <- remove_rows_na(train_x)
secondary_test_df <- remove_rows_na(test_df)
secondary_test_x <- remove_rows_na(test_x)

```

We load the evaluation dataset we will make predictions on.

```{r evaluation_data}
my_url2 <- "https://github.com/geedoubledee/data624_project2/raw/main/StudentEvaluation.xlsx"
temp <- tempfile(fileext = ".xlsx")
req <- GET(my_url2, authenticate(Sys.getenv("GITHUB_PAT"), ""),
           write_disk(path = temp))
eval_df <- readxl::read_excel(temp)
colnames(eval_df) <- to_screaming_snake_case(colnames(eval_df))

```

We build and report the predictive factors in BOTH a technical and non-technical report. The non-technical report will be in a  business-friendly readable document, and the predictions will be in an Excel readable format. The technical report will clearly show the models we tested and how we selected our final approach.
